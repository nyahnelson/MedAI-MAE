{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download section 4, 5, 7, 8\n",
    "\n",
    "#### this script combines the webscraping scripts for sections 4,5,7,8 for the ultrasoundcases.info site \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web scrape + download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1A. Section 4 web scraping + download at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_FILE = 'loop_state.json'\n",
    "\n",
    "# save place in for loops\n",
    "def save_state(loop1_iter, loop2_iter, loop3_iter, loop4_iter, loop5_iter):\n",
    "    state = {\n",
    "        'download_loop': {\n",
    "            'loop1_iter': loop1_iter,\n",
    "            'loop2_iter': loop2_iter,\n",
    "            'loop3_iter': loop3_iter,\n",
    "            'loop4_iter': loop4_iter,\n",
    "            'loop5_iter': loop5_iter,\n",
    "        }\n",
    "    }\n",
    "    with open(STATE_FILE, 'w') as state_file:\n",
    "        json.dump(state, state_file)\n",
    "\n",
    "# handle request and return as parsed HTML\n",
    "def request_data(url):\n",
    "    \"\"\"\n",
    "    Request data from HMSS website and parse with beautifulsoup.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "# download one image at a time\n",
    "def download_imgs(urllink, tgt_path, filename):\n",
    "    try: \n",
    "        # full path: tgt_path + filename\n",
    "        full_tgt_path = os.path.join(tgt_path, filename)\n",
    "        # download image to the folder path\n",
    "        urllib.request.urlretrieve(urllink, full_tgt_path)\n",
    "        print('{} saved to {}.'.format(filename, full_tgt_path))\n",
    "        return True\n",
    "    except HTTPError as e: # exception for the http error (404 not found page)\n",
    "        print(f\"Error downloading image from {urllink}: {e}\") \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_names = ['group', 'subgroup', 'subgroup url', 'case description', 'img url', 'img name', 'case id/cover img id', 'crop r1', 'crop r2', 'crop col1', 'crop col2', 'tumor type', 'Notes']\n",
    "col_names = ['group', 'subgroup', 'subgroup url', 'case description', 'img url', 'img name', 'sex', 'age', 'body part']\n",
    "\n",
    "data_4 = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Head and Neck\n",
      "Fetching 4.1 Thyroid gland 94\n",
      "Fetching 4.1.1 Thyroid congenital abnormalities 275\n",
      "Fetching Thyroid congenital abnormalities 3393 thyroid-congenital-abnormalities-267\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1463.jpg\n",
      "1463.jpg saved to imgs/1463.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1464.jpg\n",
      "1464.jpg saved to imgs/1464.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1465.jpg\n",
      "1465.jpg saved to imgs/1465.jpg.\n",
      "Fetching Congenital hypoplasia 3395 congenital-hypoplasia-3535\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/19557.jpg\n",
      "19557.jpg saved to imgs/19557.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/19558.jpg\n",
      "19558.jpg saved to imgs/19558.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/19559.jpg\n",
      "19559.jpg saved to imgs/19559.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/19560.jpg\n",
      "19560.jpg saved to imgs/19560.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/19561.jpg\n",
      "19561.jpg saved to imgs/19561.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/19562.jpg\n",
      "19562.jpg saved to imgs/19562.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/19563.jpg\n",
      "19563.jpg saved to imgs/19563.jpg.\n",
      "Fetching Congenital hypoplasia 3396 congenital-hypoplasia-3536\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/15737.jpg\n",
      "15737.jpg saved to imgs/15737.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/15738.jpg\n",
      "15738.jpg saved to imgs/15738.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/15739.jpg\n",
      "15739.jpg saved to imgs/15739.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/15740.jpg\n",
      "15740.jpg saved to imgs/15740.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/15741.jpg\n",
      "15741.jpg saved to imgs/15741.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/15742.jpg\n",
      "15742.jpg saved to imgs/15742.jpg.\n",
      "Fetching Ectopic thyroid tissue 3394 ectopic-thyroid-tissue-268\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1466.jpg\n",
      "1466.jpg saved to imgs/1466.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1467.jpg\n",
      "1467.jpg saved to imgs/1467.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1468.jpg\n",
      "1468.jpg saved to imgs/1468.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1469.jpg\n",
      "1469.jpg saved to imgs/1469.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1470.jpg\n",
      "1470.jpg saved to imgs/1470.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1471.jpg\n",
      "1471.jpg saved to imgs/1471.jpg.\n",
      "Fetching Congenital hypothyroidism 3397 congenital-hypothyroidism-3537\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/9924.jpg\n",
      "9924.jpg saved to imgs/9924.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/9925.jpg\n",
      "9925.jpg saved to imgs/9925.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/9926.jpg\n",
      "9926.jpg saved to imgs/9926.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/9927.jpg\n",
      "9927.jpg saved to imgs/9927.jpg.\n",
      "Fetching Congenital hypothyroidism 3398 congenital-hypothyroidism-3538\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/9928.jpg\n",
      "9928.jpg saved to imgs/9928.jpg.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m item_l5_img_url_src \u001b[39m=\u001b[39m item_l5[\u001b[39m\"\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     99\u001b[0m item_l5_img_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.ultrasoundcases.info\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m item_l5_img_url_src\n\u001b[0;32m--> 101\u001b[0m img_soup \u001b[39m=\u001b[39m request_data(\n\u001b[1;32m    102\u001b[0m     item_l5_img_url\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFetching\u001b[39m\u001b[39m\"\u001b[39m, item_l5_img_url)\n\u001b[1;32m    107\u001b[0m \u001b[39m# prep data for dataframe\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 21\u001b[0m, in \u001b[0;36mrequest_data\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_data\u001b[39m(url):\n\u001b[1;32m     18\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    Request data from HMSS website and parse with beautifulsoup.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m     22\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m soup\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    404\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     conn\u001b[39m.\u001b[39mconnect()\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1056\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1057\u001b[0m         (\n\u001b[1;32m   1058\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1064\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    411\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs\n\u001b[1;32m    412\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(context, \u001b[39m\"\u001b[39m\u001b[39mload_default_certs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    420\u001b[0m     sock\u001b[39m=\u001b[39mconn,\n\u001b[1;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_file,\n\u001b[1;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcert_file,\n\u001b[1;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_password,\n\u001b[1;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs,\n\u001b[1;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir,\n\u001b[1;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_data,\n\u001b[1;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39mserver_hostname,\n\u001b[1;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39mcontext,\n\u001b[1;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39mtls_in_tls,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    436\u001b[0m     default_ssl_context\n\u001b[1;32m    437\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock, \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock\u001b[39m.\u001b[39mversion() \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mTLSv1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTLSv1.1\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    440\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py:402\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m ca_certs \u001b[39mor\u001b[39;00m ca_cert_dir \u001b[39mor\u001b[39;00m ca_cert_data:\n\u001b[1;32m    401\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m         context\u001b[39m.\u001b[39mload_verify_locations(ca_certs, ca_cert_dir, ca_cert_data)\n\u001b[1;32m    403\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mIOError\u001b[39;00m, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scraping the data from HMSS website for section 4 (4.1-4.5) \n",
    "\"\"\"\n",
    "\n",
    "base_url = 'https://www.ultrasoundcases.info/'\n",
    "INIT_URL = \"https://www.ultrasoundcases.info/cases/head-and-neck/\"\n",
    "init_soup = request_data(INIT_URL)\n",
    "first_level = init_soup.find_all(\"div\", {\"class\": \"candidate-filter\"})\n",
    "tgt_path = 'imgs/'\n",
    "# make sure file directory exists\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "# loop counts\n",
    "l1_i = 0\n",
    "l2_i = 0\n",
    "l3_i = 0\n",
    "l4_i = 0\n",
    "l5_i = 0\n",
    "\n",
    "# go through all first level categories (first level = body system (EX. 4 Head and Neck))\n",
    "for item_l1 in first_level:\n",
    "    # get the title from h4>a\n",
    "    item_l1_title = item_l1.find(\"h4\").find(\"a\").text\n",
    "    # only fetch section 4 items\n",
    "    if (item_l1_title != 'Head and Neck'):\n",
    "        continue\n",
    "    print('Fetching', item_l1_title)\n",
    "    \n",
    "    # get the second level categories\n",
    "    second_level = item_l1.find(\"ul\").find_all(\"li\")\n",
    "    \n",
    "    # iterate through each item in the second level (second level = group of body system (EX. 4.1 Thyroid Gland))\n",
    "    for item_l2 in second_level:\n",
    "        item_l2_title = item_l2.find(\"a\").text\n",
    "        item_l2_url_title = item_l2.find(\"a\")['href']\n",
    "        item_l2_id = item_l2.find(\"a\")[\"data-id\"] \n",
    "\n",
    "        print(\"Fetching\", item_l2_title, item_l2_id)\n",
    "\n",
    "        third_level = requests.get(\n",
    "            \"https://www.ultrasoundcases.info/api/cases/list/\" + str(item_l2_id)\n",
    "        ).json()\n",
    "        \n",
    "        # iterate through each item in the third level (third level = subgroup of group (EX. 4.1.1 Thyroid Congenital Abnormalities))\n",
    "        for item_l3 in third_level: \n",
    "            item_l3_title = item_l3[\"title\"]\n",
    "            item_l3_urltitle = item_l3[\"urltitle\"]\n",
    "            item_l3_id = item_l3[\"id\"]\n",
    "\n",
    "            print(\"Fetching\", item_l3_title, item_l3_id)\n",
    "\n",
    "            fourth_level = requests.post(\n",
    "                \"https://www.ultrasoundcases.info/api/cases/list/\" +  str(item_l3_id) + '/',\n",
    "                data={\"type\": \"subsubcat\"}\n",
    "            ).json()\n",
    "\n",
    "            # iterate through each item in the fourth level (fourth level = specific cases of subgroup (EX. Hypoplasia of the left thyroid lobe))\n",
    "            for item_l4 in fourth_level:\n",
    "                item_l4_id = item_l4['id']\n",
    "                item_l4_title = item_l4['title']\n",
    "                item_l4_subtitle = item_l4['subtitle']\n",
    "                item_l4_thumb = item_l4['thumb']\n",
    "                item_l4_urltitle = item_l4['urltitle']\n",
    "\n",
    "                print(\"Fetching\", item_l4_title, item_l4_id, item_l4_urltitle)\n",
    "\n",
    "                item_l5_soup = request_data(\n",
    "                    \"https://www.ultrasoundcases.info/{}/\".format(item_l4_urltitle)\n",
    "                )\n",
    "\n",
    "                # img information\n",
    "                fifth_level = item_l5_soup.find(\"div\", {\"class\": \"portfolio\"})\n",
    "                fifth_level = fifth_level.find_all(\"img\")\n",
    "\n",
    "                # patient details\n",
    "                l5_details = item_l5_soup.find(\"div\", {\"class\": \"information\"})\n",
    "                l5_h4 = l5_details.find(\"h4\")\n",
    "                # check if details about case exist\n",
    "                if(l5_h4.text == 'Details'):\n",
    "                    l5_patient_info = l5_details.find_all(\"li\")\n",
    "                    for i, li in enumerate(l5_patient_info):\n",
    "                        details = ''.join(li.find_all(string=True, recursive=False)).strip()\n",
    "                        if i == 0:\n",
    "                            l5_sex = details\n",
    "                        if i == 1:\n",
    "                            l5_age = details\n",
    "                        if i == 2:\n",
    "                            l5_body_part = details\n",
    "                else:\n",
    "                    l5_sex = np.nan\n",
    "                    l5_age = np.nan\n",
    "                    l5_body_part = np.nan\n",
    "                    \n",
    "\n",
    "                # iterate through each item in the case level (fifth level = image details for the specific case)\n",
    "                for item_l5 in fifth_level:  # case level\n",
    "                    item_l5_img_url_src = item_l5[\"src\"]\n",
    "                    item_l5_img_url = \"https://www.ultrasoundcases.info\" + item_l5_img_url_src\n",
    "\n",
    "                    img_soup = request_data(\n",
    "                        item_l5_img_url\n",
    "                    )\n",
    "\n",
    "                    print(\"Fetching\", item_l5_img_url)\n",
    "\n",
    "                    # prep data for dataframe\n",
    "                    item_l3_url = base_url + item_l2_url_title + item_l3_urltitle\n",
    "                    img_name = item_l5_img_url.split('/')\n",
    "                    item_l5_img_name = img_name[-1]\n",
    "\n",
    "                    if not (item_l5_img_name.endswith(\".jpg\")):\n",
    "                        # not a jpg image, so do not add it, go to the next image\n",
    "                        continue\n",
    "\n",
    "                    # download images here\n",
    "                    if (download_imgs(item_l5_img_url, tgt_path, item_l5_img_name)):\n",
    "                        # if an image is downloaded\n",
    "                        # create row for dataframe\n",
    "                        data_4.loc[len(data_4.index)] = [item_l2_title, item_l3_title, item_l3_url, item_l4_subtitle, item_l5_img_url, item_l5_img_name, l5_sex, l5_age, l5_body_part]\n",
    "                    \n",
    "                        save_state(l1_i, l2_i, l3_i, l4_i, l5_i)\n",
    "                    l5_i += 1\n",
    "                l5_i = 0\n",
    "                l4_i += 1\n",
    "            l4_i = 0\n",
    "            l3_i += 1\n",
    "        l3_i = 0\n",
    "        l2_i += 1 \n",
    "    l2_i = 0\n",
    "    l1_i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resuming: \n",
      "Last iteration on first loop: 0\n",
      "Last iteration on first loop: 1\n",
      "Last iteration on first loop: 1\n",
      "Last iteration on first loop: 2\n",
      "Last iteration on first loop: 1\n"
     ]
    }
   ],
   "source": [
    "# open file for resume location\n",
    "with open('loop_state.json', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Parse the JSON content\n",
    "state_data = json.loads(content)\n",
    "\n",
    "# Access the values for loop iterations\n",
    "l1_res = state_data['download_loop']['loop1_iter']\n",
    "l2_res = state_data['download_loop']['loop2_iter']\n",
    "l3_res = state_data['download_loop']['loop3_iter']\n",
    "l4_res = state_data['download_loop']['loop4_iter']\n",
    "l5_res = state_data['download_loop']['loop5_iter']\n",
    "\n",
    "print(\"resuming: \")\n",
    "print(f\"Last iteration on first loop: {l1_res}\")\n",
    "print(f\"Last iteration on first loop: {l2_res}\")\n",
    "print(f\"Last iteration on first loop: {l3_res}\")\n",
    "print(f\"Last iteration on first loop: {l4_res}\")\n",
    "print(f\"Last iteration on first loop: {l5_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Head and Neck\n",
      "Fetching 4.1 Thyroid gland 94\n",
      "Fetching 4.1.1 Thyroid congenital abnormalities 275\n",
      "Fetching Thyroid congenital abnormalities 3393 thyroid-congenital-abnormalities-267\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1463.jpg\n",
      "1463.jpg saved to imgs/1463.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1464.jpg\n",
      "1464.jpg saved to imgs/1464.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/1465.jpg\n",
      "1465.jpg saved to imgs/1465.jpg.\n",
      "Fetching Congenital hypoplasia 3395 congenital-hypoplasia-3535\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/19557.jpg\n",
      "19557.jpg saved to imgs/19557.jpg.\n",
      "Fetching https://www.ultrasoundcases.info/clients/ultrasoundcases/uploads/19558.jpg\n",
      "19558.jpg saved to imgs/19558.jpg.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 245\u001b[0m\n\u001b[1;32m    242\u001b[0m item_l5_img_url_src \u001b[39m=\u001b[39m item_l5[\u001b[39m\"\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    243\u001b[0m item_l5_img_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.ultrasoundcases.info\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m item_l5_img_url_src\n\u001b[0;32m--> 245\u001b[0m img_soup \u001b[39m=\u001b[39m request_data(\n\u001b[1;32m    246\u001b[0m     item_l5_img_url\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    248\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFetching\u001b[39m\u001b[39m\"\u001b[39m, item_l5_img_url)\n\u001b[1;32m    250\u001b[0m \u001b[39m# prep data for dataframe\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 21\u001b[0m, in \u001b[0;36mrequest_data\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_data\u001b[39m(url):\n\u001b[1;32m     18\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    Request data from HMSS website and parse with beautifulsoup.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m     22\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m soup\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scraping the data from HMSS website for section 4 (4.1-4.5) \n",
    "\"\"\"\n",
    "\n",
    "base_url = 'https://www.ultrasoundcases.info/'\n",
    "INIT_URL = \"https://www.ultrasoundcases.info/cases/head-and-neck/\"\n",
    "init_soup = request_data(INIT_URL)\n",
    "first_level = init_soup.find_all(\"div\", {\"class\": \"candidate-filter\"})\n",
    "tgt_path = 'imgs/'\n",
    "# make sure file directory exists\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "# loop counts\n",
    "l1_i = 0\n",
    "l2_i = 0\n",
    "l3_i = 0\n",
    "l4_i = 0\n",
    "l5_i = 0\n",
    "\n",
    "l1_resume = True\n",
    "l2_resume = True\n",
    "l3_resume = True\n",
    "l4_resume = True\n",
    "l5_resume = True\n",
    "\n",
    "# go through all first level categories (first level = body system (EX. 4 Head and Neck))\n",
    "for item_l1 in first_level:\n",
    "    if (l1_resume == True):\n",
    "        if (l1_i != l1_res): # not the same place left off, so loop again\n",
    "            print(f\"on loop {l1_i}, looping again until loop {l1_res}\")\n",
    "            l2_i = 0\n",
    "            l1_i += 1\n",
    "            continue\n",
    "        else:\n",
    "            l1_resume = False\n",
    "            # get the title from h4>a\n",
    "            item_l1_title = item_l1.find(\"h4\").find(\"a\").text\n",
    "            # only fetch section 4 items\n",
    "            if (item_l1_title != 'Head and Neck'):\n",
    "                continue\n",
    "            print('Fetching', item_l1_title)\n",
    "            \n",
    "            # get the second level categories\n",
    "            second_level = item_l1.find(\"ul\").find_all(\"li\")\n",
    "            l2_i = 0\n",
    "            l1_i += 1\n",
    "    else:\n",
    "        # get the title from h4>a\n",
    "        item_l1_title = item_l1.find(\"h4\").find(\"a\").text\n",
    "        # only fetch section 4 items\n",
    "        if (item_l1_title != 'Head and Neck'):\n",
    "            continue\n",
    "        print('Fetching', item_l1_title)\n",
    "        \n",
    "        # get the second level categories\n",
    "        second_level = item_l1.find(\"ul\").find_all(\"li\")\n",
    "    \n",
    "    # iterate through each item in the second level (second level = group of body system (EX. 4.1 Thyroid Gland))\n",
    "    for item_l2 in second_level:\n",
    "        if (l2_resume == True):\n",
    "            if (l2_i != l2_res): # not the same place left off, so loop again\n",
    "                print(f\"on loop {l2_i}, looping again until loop {l2_res}\")\n",
    "                l3_i = 0\n",
    "                l2_i += 1\n",
    "                continue\n",
    "            else:\n",
    "                l2_resume = False\n",
    "                item_l2_title = item_l2.find(\"a\").text\n",
    "                item_l2_url_title = item_l2.find(\"a\")['href']\n",
    "                item_l2_id = item_l2.find(\"a\")[\"data-id\"] \n",
    "\n",
    "                print(\"Fetching\", item_l2_title, item_l2_id)\n",
    "\n",
    "                third_level = requests.get(\n",
    "                    \"https://www.ultrasoundcases.info/api/cases/list/\" + str(item_l2_id)\n",
    "                ).json()\n",
    "                l3_i = 0\n",
    "                l2_i += 1\n",
    "        else:\n",
    "            item_l2_title = item_l2.find(\"a\").text\n",
    "            item_l2_url_title = item_l2.find(\"a\")['href']\n",
    "            item_l2_id = item_l2.find(\"a\")[\"data-id\"] \n",
    "\n",
    "            print(\"Fetching\", item_l2_title, item_l2_id)\n",
    "\n",
    "            third_level = requests.get(\n",
    "                \"https://www.ultrasoundcases.info/api/cases/list/\" + str(item_l2_id)\n",
    "            ).json()\n",
    "        \n",
    "        # iterate through each item in the third level (third level = subgroup of group (EX. 4.1.1 Thyroid Congenital Abnormalities))\n",
    "        for item_l3 in third_level: \n",
    "            if (l3_resume == True):\n",
    "                if (l3_i != l3_res): # not the same place left off, so loop again\n",
    "                    print(f\"on loop {l3_i}, looping again until loop {l3_res}\")\n",
    "                    l4_i = 0\n",
    "                    l3_i += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    l3_resume = False\n",
    "                    item_l3_title = item_l3[\"title\"]\n",
    "                    item_l3_urltitle = item_l3[\"urltitle\"]\n",
    "                    item_l3_id = item_l3[\"id\"]\n",
    "\n",
    "                    print(\"Fetching\", item_l3_title, item_l3_id)\n",
    "\n",
    "                    fourth_level = requests.post(\n",
    "                        \"https://www.ultrasoundcases.info/api/cases/list/\" +  str(item_l3_id) + '/',\n",
    "                        data={\"type\": \"subsubcat\"}\n",
    "                    ).json()\n",
    "                    l4_i =0\n",
    "                    l3_i += 1\n",
    "            else:\n",
    "                item_l3_title = item_l3[\"title\"]\n",
    "                item_l3_urltitle = item_l3[\"urltitle\"]\n",
    "                item_l3_id = item_l3[\"id\"]\n",
    "\n",
    "                print(\"Fetching\", item_l3_title, item_l3_id)\n",
    "\n",
    "                fourth_level = requests.post(\n",
    "                    \"https://www.ultrasoundcases.info/api/cases/list/\" +  str(item_l3_id) + '/',\n",
    "                    data={\"type\": \"subsubcat\"}\n",
    "                ).json()\n",
    "\n",
    "            # iterate through each item in the fourth level (fourth level = specific cases of subgroup (EX. Hypoplasia of the left thyroid lobe))\n",
    "            for item_l4 in fourth_level:\n",
    "                if(l4_resume == True):\n",
    "                    if (l4_i != l4_res): # not the same place left off, so loop again\n",
    "                        print(f\"on loop {l4_i}, looping again until loop {l4_res}\")\n",
    "                        l5_i = 0\n",
    "                        l4_i += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        l4_resume = False\n",
    "                        item_l4_id = item_l4['id']\n",
    "                        item_l4_title = item_l4['title']\n",
    "                        item_l4_subtitle = item_l4['subtitle']\n",
    "                        item_l4_thumb = item_l4['thumb']\n",
    "                        item_l4_urltitle = item_l4['urltitle']\n",
    "\n",
    "                        print(\"Fetching\", item_l4_title, item_l4_id, item_l4_urltitle)\n",
    "\n",
    "                        item_l5_soup = request_data(\n",
    "                            \"https://www.ultrasoundcases.info/{}/\".format(item_l4_urltitle)\n",
    "                        )\n",
    "                        # img information\n",
    "                        fifth_level = item_l5_soup.find(\"div\", {\"class\": \"portfolio\"})\n",
    "                        fifth_level = fifth_level.find_all(\"img\")\n",
    "\n",
    "                        # patient details\n",
    "                        l5_details = item_l5_soup.find(\"div\", {\"class\": \"information\"})\n",
    "                        l5_h4 = l5_details.find(\"h4\")\n",
    "                        # check if details about case exist\n",
    "                        if(l5_h4.text == 'Details'):\n",
    "                            l5_patient_info = l5_details.find_all(\"li\")\n",
    "                            for i, li in enumerate(l5_patient_info):\n",
    "                                details = ''.join(li.find_all(string=True, recursive=False)).strip()\n",
    "                                if i == 0:\n",
    "                                    l5_sex = details\n",
    "                                if i == 1:\n",
    "                                    l5_age = details\n",
    "                                if i == 2:\n",
    "                                    l5_body_part = details\n",
    "                        else:\n",
    "                            l5_sex = np.nan\n",
    "                            l5_age = np.nan\n",
    "                            l5_body_part = np.nan\n",
    "                        l5_i = 0\n",
    "                        l4_i += 1\n",
    "                else:\n",
    "                    item_l4_id = item_l4['id']\n",
    "                    item_l4_title = item_l4['title']\n",
    "                    item_l4_subtitle = item_l4['subtitle']\n",
    "                    item_l4_thumb = item_l4['thumb']\n",
    "                    item_l4_urltitle = item_l4['urltitle']\n",
    "\n",
    "                    print(\"Fetching\", item_l4_title, item_l4_id, item_l4_urltitle)\n",
    "\n",
    "                    item_l5_soup = request_data(\n",
    "                        \"https://www.ultrasoundcases.info/{}/\".format(item_l4_urltitle)\n",
    "                    )\n",
    "\n",
    "                    # img information\n",
    "                    fifth_level = item_l5_soup.find(\"div\", {\"class\": \"portfolio\"})\n",
    "                    fifth_level = fifth_level.find_all(\"img\")\n",
    "\n",
    "                    # patient details\n",
    "                    l5_details = item_l5_soup.find(\"div\", {\"class\": \"information\"})\n",
    "                    l5_h4 = l5_details.find(\"h4\")\n",
    "                    # check if details about case exist\n",
    "                    if(l5_h4.text == 'Details'):\n",
    "                        l5_patient_info = l5_details.find_all(\"li\")\n",
    "                        for i, li in enumerate(l5_patient_info):\n",
    "                            details = ''.join(li.find_all(string=True, recursive=False)).strip()\n",
    "                            if i == 0:\n",
    "                                l5_sex = details\n",
    "                            if i == 1:\n",
    "                                l5_age = details\n",
    "                            if i == 2:\n",
    "                                l5_body_part = details\n",
    "                    else:\n",
    "                        l5_sex = np.nan\n",
    "                        l5_age = np.nan\n",
    "                        l5_body_part = np.nan\n",
    "\n",
    "                # iterate through each item in the case level (fifth level = image details for the specific case)\n",
    "                for item_l5 in fifth_level:  # case level\n",
    "                    if (l5_resume == True):\n",
    "                        if (l5_i != l5_res): # not the same place left off, so loop again\n",
    "                            print(f\"on loop {l5_i}, looping again until loop {l5_res}\")\n",
    "                            l5_i += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            l5_resume = False\n",
    "                            item_l5_img_url_src = item_l5[\"src\"]\n",
    "                            item_l5_img_url = \"https://www.ultrasoundcases.info\" + item_l5_img_url_src\n",
    "\n",
    "                            img_soup = request_data(\n",
    "                                item_l5_img_url\n",
    "                            )\n",
    "\n",
    "                            print(\"Fetching\", item_l5_img_url)\n",
    "\n",
    "                            # prep data for dataframe\n",
    "                            item_l3_url = base_url + item_l2_url_title + item_l3_urltitle\n",
    "                            img_name = item_l5_img_url.split('/')\n",
    "                            item_l5_img_name = img_name[-1]\n",
    "\n",
    "                            if not (item_l5_img_name.endswith(\".jpg\")):\n",
    "                                # not a jpg image, so do not add it, go to the next image\n",
    "                                continue\n",
    "\n",
    "                            # download images here\n",
    "                            if (download_imgs(item_l5_img_url, tgt_path, item_l5_img_name)):\n",
    "                                # if an image is downloaded\n",
    "                                # create row for dataframe\n",
    "                                data_4.loc[len(data_4.index)] = [item_l2_title, item_l3_title, item_l3_url, item_l4_subtitle, item_l5_img_url, item_l5_img_name, l5_sex, l5_age, l5_body_part]\n",
    "                            \n",
    "                                save_state(l1_i, l2_i, l3_i, l4_i, l5_i)\n",
    "                            l5_i += 1\n",
    "                    else:\n",
    "                        item_l5_img_url_src = item_l5[\"src\"]\n",
    "                        item_l5_img_url = \"https://www.ultrasoundcases.info\" + item_l5_img_url_src\n",
    "\n",
    "                        img_soup = request_data(\n",
    "                            item_l5_img_url\n",
    "                        )\n",
    "                        print(\"Fetching\", item_l5_img_url)\n",
    "\n",
    "                        # prep data for dataframe\n",
    "                        item_l3_url = base_url + item_l2_url_title + item_l3_urltitle\n",
    "                        img_name = item_l5_img_url.split('/')\n",
    "                        item_l5_img_name = img_name[-1]\n",
    "                            \n",
    "                        if not (item_l5_img_name.endswith(\".jpg\")):\n",
    "                            # not a jpg image, so do not add it, go to the next image\n",
    "                            continue\n",
    "\n",
    "                        # download images here\n",
    "                        if (download_imgs(item_l5_img_url, tgt_path, item_l5_img_name)):\n",
    "                            # if an image is downloaded\n",
    "                            # create row for dataframe\n",
    "                            data_4.loc[len(data_4.index)] = [item_l2_title, item_l3_title, item_l3_url, item_l4_subtitle, item_l5_img_url, item_l5_img_name, l5_sex, l5_age, l5_body_part]\n",
    "                            \n",
    "                            save_state(l1_i, l2_i, l3_i, l4_i, l5_i)\n",
    "                        # at the end of the 2nd else in loop 5\n",
    "                        l5_i += 1\n",
    "                l5_i = 0\n",
    "                l4_i += 1\n",
    "            l4_i = 0\n",
    "            l3_i += 1\n",
    "        l3_i = 0\n",
    "        l2_i += 1 \n",
    "    l2_i = 0\n",
    "    l1_i += 1\n",
    "print(\"-------------------FINISHED SECTION 4 DOWNLOAD----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to csv file\n",
    "data_4.to_csv('section4/section4.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B. Section 5 web scraping + download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5 = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scraping the data from HMSS website for section 5 (5.1-5.8)\n",
    "\"\"\"\n",
    "\n",
    "base_url = 'https://www.ultrasoundcases.info/'\n",
    "INIT_URL = \"https://www.ultrasoundcases.info/cases/breast-and-axilla/\"\n",
    "init_soup = request_data(INIT_URL)\n",
    "first_level = init_soup.find_all(\"div\", {\"class\": \"candidate-filter\"})\n",
    "tgt_path = 'imgs/'\n",
    "benign_path = 'section5/imgs/benign'\n",
    "malig_path = 'section5/imgs/malignant'\n",
    "# make sure file directory exists\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "if not os.path.exists(benign_path):\n",
    "    os.makedirs(benign_path)\n",
    "\n",
    "if not os.path.exists(malig_path):\n",
    "    os.makedirs(malig_path)\n",
    "\n",
    "# go through all first level categories (first level = body system (EX. 5 Breast and Axilla))\n",
    "for item_l1 in first_level:\n",
    "    # get the title from h4>a\n",
    "    item_l1_title = item_l1.find(\"h4\").find(\"a\").text\n",
    "    # only fetch section 4 items\n",
    "    if (item_l1_title != 'Breast and Axilla'):\n",
    "        continue\n",
    "    print('Fetching', item_l1_title)\n",
    "    \n",
    "    # get the second level categories\n",
    "    second_level = item_l1.find(\"ul\").find_all(\"li\")\n",
    "    \n",
    "    # iterate through each item in the second level (second level = group of body system (EX. 5.1 Benign Lesions))\n",
    "    for item_l2 in second_level:\n",
    "        item_l2_title = item_l2.find(\"a\").text\n",
    "        item_l2_url_title = item_l2.find(\"a\")['href']\n",
    "        item_l2_id = item_l2.find(\"a\")[\"data-id\"] \n",
    "\n",
    "        print(\"Fetching\", item_l2_title, item_l2_id)\n",
    "\n",
    "        third_level = requests.get(\n",
    "            \"https://www.ultrasoundcases.info/api/cases/list/\" + str(item_l2_id)\n",
    "        ).json()\n",
    "        \n",
    "        # iterate through each item in the third level (third level = subgroup of group (EX. 5.1.1 Benign cysts with non mass lesions))\n",
    "        for item_l3 in third_level: \n",
    "            item_l3_title = item_l3[\"title\"]\n",
    "            item_l3_urltitle = item_l3[\"urltitle\"]\n",
    "            item_l3_id = item_l3[\"id\"]\n",
    "\n",
    "            print(\"Fetching\", item_l3_title, item_l3_id)\n",
    "\n",
    "            fourth_level = requests.post(\n",
    "                \"https://www.ultrasoundcases.info/api/cases/list/\" +  str(item_l3_id) + '/',\n",
    "                data={\"type\": \"subsubcat\"}\n",
    "            ).json()\n",
    "\n",
    "            # iterate through each item in the fourth level (fourth level = specific cases of subgroup (EX. Benign cysts with non mass lesions))\n",
    "            for item_l4 in fourth_level:\n",
    "                item_l4_id = item_l4['id']\n",
    "                item_l4_title = item_l4['title']\n",
    "                item_l4_subtitle = item_l4['subtitle']\n",
    "                item_l4_thumb = item_l4['thumb']\n",
    "                item_l4_urltitle = item_l4['urltitle']\n",
    "\n",
    "                print(\"Fetching\", item_l4_title, item_l4_id, item_l4_urltitle)\n",
    "\n",
    "                item_l5_soup = request_data(\n",
    "                    \"https://www.ultrasoundcases.info/{}/\".format(item_l4_urltitle)\n",
    "                )\n",
    "\n",
    "                # img information\n",
    "                fifth_level = item_l5_soup.find(\"div\", {\"class\": \"portfolio\"})\n",
    "                fifth_level = fifth_level.find_all(\"img\")\n",
    "\n",
    "                # patient details\n",
    "                l5_details = item_l5_soup.find(\"div\", {\"class\": \"information\"})\n",
    "                l5_h4 = l5_details.find(\"h4\")\n",
    "                \n",
    "                # check if details about case exist\n",
    "                if(l5_h4.text == 'Details'):\n",
    "                    l5_patient_info = l5_details.find_all(\"li\")\n",
    "                    for i, li in enumerate(l5_patient_info):\n",
    "                        details = ''.join(li.find_all(string=True, recursive=False)).strip()\n",
    "                        if i == 0:\n",
    "                            l5_sex = details\n",
    "                        if i == 1:\n",
    "                            l5_age = details\n",
    "                        if i == 2:\n",
    "                            l5_body_part = details\n",
    "                else:\n",
    "                    l5_sex = np.nan\n",
    "                    l5_age = np.nan\n",
    "                    l5_body_part = np.nan\n",
    "                    \n",
    "\n",
    "                # iterate through each item in the case level (fifth level = image details for the specific case)\n",
    "                for item_l5 in fifth_level:  # case level\n",
    "                    item_l5_img_url_src = item_l5[\"src\"]\n",
    "                    item_l5_img_url = \"https://www.ultrasoundcases.info\" + item_l5_img_url_src\n",
    "\n",
    "                    img_soup = request_data(\n",
    "                        item_l5_img_url\n",
    "                    )\n",
    "\n",
    "                    print(\"Fetching\", item_l5_img_url)\n",
    "\n",
    "                    # prep data for dataframe\n",
    "                    item_l3_url = base_url + item_l2_url_title + item_l3_urltitle\n",
    "                    img_name = item_l5_img_url.split('/')\n",
    "                    item_l5_img_name = img_name[-1]\n",
    "\n",
    "                    if not (item_l5_img_name.endswith(\".jpg\")):\n",
    "                        # not a jpg image, so do not add it, go to the next image\n",
    "                        continue\n",
    "\n",
    "                    # download images here\n",
    "                    if (download_imgs(item_l5_img_url, tgt_path, item_l5_img_name)):\n",
    "                        # if an image is downloaded\n",
    "                        # create row for dataframe\n",
    "                        data_5.loc[len(data_5.index)] = [item_l2_title, item_l3_title, item_l3_url, item_l4_subtitle, item_l5_img_url, item_l5_img_name, l5_sex, l5_age, l5_body_part]\n",
    "                        if (item_l2_title == '5.1 Benign lesions'):\n",
    "                            # download image to benign folder\n",
    "                            download_imgs(item_l5_img_url, benign_path, item_l5_img_name)\n",
    "                        elif (item_l2_title == '5.2 Malignant breast lesions'):\n",
    "                            # download image to malignant folder\n",
    "                            download_imgs(item_l5_img_url, malig_path, item_l5_img_name)\n",
    "                        else:\n",
    "                            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to csv file\n",
    "data_5.to_csv('section5/section5.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1C. Section 7 web scraping + download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_7 = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scraping the data from HMSS website for section 7 (7.2-7.5)\n",
    "\"\"\"\n",
    "\n",
    "base_url = 'https://www.ultrasoundcases.info/'\n",
    "INIT_URL = \"https://www.ultrasoundcases.info/cases/musculo-skeletal-bone-muscle-nerves-and-other-soft-tissues/\"\n",
    "init_soup = request_data(INIT_URL)\n",
    "first_level = init_soup.find_all(\"div\", {\"class\": \"candidate-filter\"})\n",
    "tgt_path = 'imgs/'\n",
    "# make sure file directory exists\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "# go through all first level categories (first level = body system (EX. 7 Musculoskeletal, bone, muscle, nerves and other soft tissues))\n",
    "for item_l1 in first_level:\n",
    "    # get the title from h4>a\n",
    "    item_l1_title = item_l1.find(\"h4\").find(\"a\").text\n",
    "    # only fetch section 4 items\n",
    "    if (item_l1_title != 'Musculoskeletal, bone, muscle, nerves and other soft tissues'):\n",
    "        continue\n",
    "    print('Fetching', item_l1_title)\n",
    "    \n",
    "    # get the second level categories\n",
    "    second_level = item_l1.find(\"ul\").find_all(\"li\")\n",
    "    \n",
    "    # iterate through each item in the second level (second level = group of body system (EX. 7.2 Muscle))\n",
    "    for item_l2 in second_level:\n",
    "        item_l2_title = item_l2.find(\"a\").text\n",
    "        item_l2_url_title = item_l2.find(\"a\")['href']\n",
    "        item_l2_id = item_l2.find(\"a\")[\"data-id\"] \n",
    "        \n",
    "        # skip section 7.1\n",
    "        if item_l2_title == '7.1 Bone':\n",
    "            continue\n",
    "\n",
    "        print(\"Fetching\", item_l2_title, item_l2_id)\n",
    "\n",
    "        third_level = requests.get(\n",
    "            \"https://www.ultrasoundcases.info/api/cases/list/\" + str(item_l2_id)\n",
    "        ).json()\n",
    "        \n",
    "        # iterate through each item in the third level (third level = subgroup of group (EX. 7.2.1 Muscle ruptures lower extremity upper leg))\n",
    "        for item_l3 in third_level: \n",
    "            item_l3_title = item_l3[\"title\"]\n",
    "            item_l3_urltitle = item_l3[\"urltitle\"]\n",
    "            item_l3_id = item_l3[\"id\"]\n",
    "\n",
    "            print(\"Fetching\", item_l3_title, item_l3_id)\n",
    "\n",
    "            fourth_level = requests.post(\n",
    "                \"https://www.ultrasoundcases.info/api/cases/list/\" +  str(item_l3_id) + '/',\n",
    "                data={\"type\": \"subsubcat\"}\n",
    "            ).json()\n",
    "\n",
    "            # iterate through each item in the fourth level (fourth level = specific cases of subgroup (EX. Lower extremety: upper leg))\n",
    "            for item_l4 in fourth_level:\n",
    "                item_l4_id = item_l4['id']\n",
    "                item_l4_title = item_l4['title']\n",
    "                item_l4_subtitle = item_l4['subtitle']\n",
    "                item_l4_thumb = item_l4['thumb']\n",
    "                item_l4_urltitle = item_l4['urltitle']\n",
    "\n",
    "                print(\"Fetching\", item_l4_title, item_l4_id, item_l4_urltitle)\n",
    "\n",
    "                item_l5_soup = request_data(\n",
    "                    \"https://www.ultrasoundcases.info/{}/\".format(item_l4_urltitle)\n",
    "                )\n",
    "\n",
    "                # img information\n",
    "                fifth_level = item_l5_soup.find(\"div\", {\"class\": \"portfolio\"})\n",
    "                fifth_level = fifth_level.find_all(\"img\")\n",
    "\n",
    "                # patient details\n",
    "                l5_details = item_l5_soup.find(\"div\", {\"class\": \"information\"})\n",
    "                l5_h4 = l5_details.find(\"h4\")\n",
    "\n",
    "                # check if details about case exist\n",
    "                if(l5_h4.text == 'Details'):\n",
    "                    l5_patient_info = l5_details.find_all(\"li\")\n",
    "                    for i, li in enumerate(l5_patient_info):\n",
    "                        details = ''.join(li.find_all(string=True, recursive=False)).strip()\n",
    "                        if i == 0:\n",
    "                            l5_sex = details\n",
    "                        if i == 1:\n",
    "                            l5_age = details\n",
    "                        if i == 2:\n",
    "                            l5_body_part = details\n",
    "                else:\n",
    "                    l5_sex = np.nan\n",
    "                    l5_age = np.nan\n",
    "                    l5_body_part = np.nan\n",
    "                    \n",
    "\n",
    "                # iterate through each item in the case level (fifth level = image details for the specific case)\n",
    "                for item_l5 in fifth_level:  # case level\n",
    "                    item_l5_img_url_src = item_l5[\"src\"]\n",
    "                    item_l5_img_url = \"https://www.ultrasoundcases.info\" + item_l5_img_url_src\n",
    "\n",
    "                    img_soup = request_data(\n",
    "                        item_l5_img_url\n",
    "                    )\n",
    "\n",
    "                    print(\"Fetching\", item_l5_img_url)\n",
    "\n",
    "                    # prep data for dataframe\n",
    "                    item_l3_url = base_url + item_l2_url_title + item_l3_urltitle\n",
    "                    img_name = item_l5_img_url.split('/')\n",
    "                    item_l5_img_name = img_name[-1]\n",
    "\n",
    "                    if not (item_l5_img_name.endswith(\".jpg\")):\n",
    "                        # not a jpg image, so do not add it, go to the next image\n",
    "                        continue\n",
    "\n",
    "                    # download images here\n",
    "                    if (download_imgs(item_l5_img_url, tgt_path, item_l5_img_name)):\n",
    "                        # if an image is downloaded\n",
    "                        # create row for dataframe\n",
    "                        data_7.loc[len(data_7.index)] = [item_l2_title, item_l3_title, item_l3_url, item_l4_subtitle, item_l5_img_url, item_l5_img_name, l5_sex, l5_age, l5_body_part]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to csv file\n",
    "data_7.to_csv('section7/section7.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D. Section 8 web scraping + download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_8 = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scraping the data from HMSS website for section 8 (8.1-8.4)\n",
    "\"\"\"\n",
    "\n",
    "base_url = 'https://www.ultrasoundcases.info/'\n",
    "INIT_URL = \"https://www.ultrasoundcases.info/cases/thorax/\"\n",
    "init_soup = request_data(INIT_URL)\n",
    "first_level = init_soup.find_all(\"div\", {\"class\": \"candidate-filter\"})\n",
    "tgt_path = 'imgs/'\n",
    "# make sure file directory exists\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "# go through all first level categories (first level = body system (EX. 8 Thorax))\n",
    "for item_l1 in first_level:\n",
    "    # get the title from h4>a\n",
    "    item_l1_title = item_l1.find(\"h4\").find(\"a\").text\n",
    "    # only fetch section 4 items\n",
    "    if (item_l1_title != 'Thorax'):\n",
    "        continue\n",
    "    print('Fetching', item_l1_title)\n",
    "    \n",
    "    # get the second level categories\n",
    "    second_level = item_l1.find(\"ul\").find_all(\"li\")\n",
    "    \n",
    "    # iterate through each item in the second level (second level = group of body system (EX. 8.1 Pulmonary pathology))\n",
    "    for item_l2 in second_level:\n",
    "        item_l2_title = item_l2.find(\"a\").text\n",
    "        item_l2_url_title = item_l2.find(\"a\")['href']\n",
    "        item_l2_id = item_l2.find(\"a\")[\"data-id\"] \n",
    "\n",
    "        print(\"Fetching\", item_l2_title, item_l2_id)\n",
    "\n",
    "        third_level = requests.get(\n",
    "            \"https://www.ultrasoundcases.info/api/cases/list/\" + str(item_l2_id)\n",
    "        ).json()\n",
    "        \n",
    "        # iterate through each item in the third level (third level = subgroup of group (EX. 8.1.1 Pneumonia and air space consolidation))\n",
    "        for item_l3 in third_level: \n",
    "            item_l3_title = item_l3[\"title\"]\n",
    "            item_l3_urltitle = item_l3[\"urltitle\"]\n",
    "            item_l3_id = item_l3[\"id\"]\n",
    "\n",
    "            print(\"Fetching\", item_l3_title, item_l3_id)\n",
    "\n",
    "            fourth_level = requests.post(\n",
    "                \"https://www.ultrasoundcases.info/api/cases/list/\" +  str(item_l3_id) + '/',\n",
    "                data={\"type\": \"subsubcat\"}\n",
    "            ).json()\n",
    "\n",
    "            # iterate through each item in the fourth level (fourth level = specific cases of subgroup (EX. Pneumonia and air space consolidation))\n",
    "            for item_l4 in fourth_level:\n",
    "                item_l4_id = item_l4['id']\n",
    "                item_l4_title = item_l4['title']\n",
    "                item_l4_subtitle = item_l4['subtitle']\n",
    "                item_l4_thumb = item_l4['thumb']\n",
    "                item_l4_urltitle = item_l4['urltitle']\n",
    "\n",
    "                print(\"Fetching\", item_l4_title, item_l4_id, item_l4_urltitle)\n",
    "\n",
    "                item_l5_soup = request_data(\n",
    "                    \"https://www.ultrasoundcases.info/{}/\".format(item_l4_urltitle)\n",
    "                )\n",
    "\n",
    "                # img information\n",
    "                fifth_level = item_l5_soup.find(\"div\", {\"class\": \"portfolio\"})\n",
    "                fifth_level = fifth_level.find_all(\"img\")\n",
    "\n",
    "                # patient details\n",
    "                l5_details = item_l5_soup.find(\"div\", {\"class\": \"information\"})\n",
    "                l5_h4 = l5_details.find(\"h4\")\n",
    "                \n",
    "                # check if details about case exist\n",
    "                if(l5_h4.text == 'Details'):\n",
    "                    l5_patient_info = l5_details.find_all(\"li\")\n",
    "                    for i, li in enumerate(l5_patient_info):\n",
    "                        details = ''.join(li.find_all(string=True, recursive=False)).strip()\n",
    "                        if i == 0:\n",
    "                            l5_sex = details\n",
    "                        if i == 1:\n",
    "                            l5_age = details\n",
    "                        if i == 2:\n",
    "                            l5_body_part = details\n",
    "                else:\n",
    "                    l5_sex = np.nan\n",
    "                    l5_age = np.nan\n",
    "                    l5_body_part = np.nan\n",
    "                    \n",
    "\n",
    "                # iterate through each item in the case level (fifth level = image details for the specific case)\n",
    "                for item_l5 in fifth_level:  # case level\n",
    "                    item_l5_img_url_src = item_l5[\"src\"]\n",
    "                    item_l5_img_url = \"https://www.ultrasoundcases.info\" + item_l5_img_url_src\n",
    "\n",
    "                    img_soup = request_data(\n",
    "                        item_l5_img_url\n",
    "                    )\n",
    "\n",
    "                    print(\"Fetching\", item_l5_img_url)\n",
    "\n",
    "                    # prep data for dataframe\n",
    "                    item_l3_url = base_url + item_l2_url_title + item_l3_urltitle\n",
    "                    img_name = item_l5_img_url.split('/')\n",
    "                    item_l5_img_name = img_name[-1]\n",
    "                    \n",
    "                    if not (item_l5_img_name.endswith(\".jpg\")):\n",
    "                        # not a jpg image, so do not add it, go to the next image\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    # download images here\n",
    "                    if (download_imgs(item_l5_img_url, tgt_path, item_l5_img_name)): \n",
    "                        # if an image is downloaded\n",
    "                        # create row for dataframe\n",
    "                        data_8.loc[len(data_8.index)] = [item_l2_title, item_l3_title, item_l3_url, item_l4_subtitle, item_l5_img_url, item_l5_img_name, l5_sex, l5_age, l5_body_part]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to csv file\n",
    "data_8.to_csv('section8/section8.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: combine .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# section 4 csv\n",
    "section4 = 'section4/section4.csv'\n",
    "df_4 = pd.read_csv(section4)\n",
    "#df_4.head()\n",
    "\n",
    "# section 5 csv\n",
    "section5 = 'section5/section5.csv'\n",
    "df_5 = pd.read_csv(section5)\n",
    "\n",
    "# section 7 csv\n",
    "section7 = 'section7/section7.csv'\n",
    "df_7 = pd.read_csv(section7)\n",
    "\n",
    "# section 8 csv\n",
    "section8 = 'section8/section8.csv'\n",
    "df_8 = pd.read_csv(section8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the dataframes to one dataframe\n",
    "df_all = [df_4, df_5, df_7, df_8]\n",
    "df = pd.DataFrame()\n",
    "df = pd.concat(df_all, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image folders and load image urls from HMSS.csv\n",
    "tgt_path = 'imgs/'\n",
    "\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "urllinkimage = df['img url']\n",
    "\n",
    "img_names = df['img name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download images\n",
    "def download_imgs(urllink, tgt_path, names):\n",
    "    for i, img_url in enumerate(urllink.values ):\n",
    "        filename = '{}'.format(names[i])\n",
    "        full_tgt_path = '{}{}'.format(tgt_path, filename)\n",
    "        print(img_url)\n",
    "        urllib.request.urlretrieve(img_url, full_tgt_path)\n",
    "        \n",
    "        print('{} - {} saved to {}.'.format(i, filename, full_tgt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download_imgs(urllinkimage, tgt_path, img_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "946e85727053cf3aa9c9f3f7fedca17119c324c882ebd65fbc35521c9647d1eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
