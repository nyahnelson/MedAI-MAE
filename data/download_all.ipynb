{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download section 4, 5, 7, 8\n",
    "\n",
    "#### this script combines the webscraping scripts for sections 4,5,7,8 for the ultrasoundcases.info site \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web scrape + download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1A. Section 4 web scraping + download at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle request and return as parsed HTML\n",
    "def request_data(url):\n",
    "    \"\"\"\n",
    "    Request data from HMSS website and parse with beautifulsoup.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "# download one image at a time\n",
    "def download_imgs(urllink, tgt_path, filename):\n",
    "    try: \n",
    "        # full path: tgt_path + filename\n",
    "        full_tgt_path = os.path.join(tgt_path, filename)\n",
    "        # download image to the folder path\n",
    "        urllib.request.urlretrieve(urllink, full_tgt_path)\n",
    "        print('{} saved to {}.'.format(filename, full_tgt_path))\n",
    "        return True\n",
    "    except HTTPError as e: # exception for the http error (404 not found page)\n",
    "        print(f\"Error downloading image from {urllink}: {e}\") \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_names = ['group', 'subgroup', 'subgroup url', 'case description', 'img url', 'img name', 'case id/cover img id', 'crop r1', 'crop r2', 'crop col1', 'crop col2', 'tumor type', 'Notes']\n",
    "col_names = ['group', 'subgroup', 'subgroup url', 'case description', 'img url', 'img name', 'sex', 'age', 'body part']\n",
    "\n",
    "data_4 = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scraping the data from HMSS website for section 4 (4.1-4.5) \n",
    "\"\"\"\n",
    "\n",
    "base_url = 'https://www.ultrasoundcases.info/'\n",
    "INIT_URL = \"https://www.ultrasoundcases.info/cases/head-and-neck/\"\n",
    "init_soup = request_data(INIT_URL)\n",
    "first_level = init_soup.find_all(\"div\", {\"class\": \"candidate-filter\"})\n",
    "tgt_path = 'imgs/'\n",
    "# make sure file directory exists\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "# go through all first level categories (first level = body system (EX. 4 Head and Neck))\n",
    "for item_l1 in first_level:\n",
    "    # get the title from h4>a\n",
    "    item_l1_title = item_l1.find(\"h4\").find(\"a\").text\n",
    "    # only fetch section 4 items\n",
    "    if (item_l1_title != 'Head and Neck'):\n",
    "        continue\n",
    "    print('Fetching', item_l1_title)\n",
    "    \n",
    "    # get the second level categories\n",
    "    second_level = item_l1.find(\"ul\").find_all(\"li\")\n",
    "    \n",
    "    # iterate through each item in the second level (second level = group of body system (EX. 4.1 Thyroid Gland))\n",
    "    for item_l2 in second_level:\n",
    "        item_l2_title = item_l2.find(\"a\").text\n",
    "        item_l2_url_title = item_l2.find(\"a\")['href']\n",
    "        item_l2_id = item_l2.find(\"a\")[\"data-id\"] \n",
    "\n",
    "        print(\"Fetching\", item_l2_title, item_l2_id)\n",
    "\n",
    "        third_level = requests.get(\n",
    "            \"https://www.ultrasoundcases.info/api/cases/list/\" + str(item_l2_id)\n",
    "        ).json()\n",
    "        \n",
    "        # iterate through each item in the third level (third level = subgroup of group (EX. 4.1.1 Thyroid Congenital Abnormalities))\n",
    "        for item_l3 in third_level: \n",
    "            item_l3_title = item_l3[\"title\"]\n",
    "            item_l3_urltitle = item_l3[\"urltitle\"]\n",
    "            item_l3_id = item_l3[\"id\"]\n",
    "\n",
    "            #print(\"Fetching\", item_l3_title, item_l3_id)\n",
    "\n",
    "            fourth_level = requests.post(\n",
    "                \"https://www.ultrasoundcases.info/api/cases/list/\" +  str(item_l3_id) + '/',\n",
    "                data={\"type\": \"subsubcat\"}\n",
    "            ).json()\n",
    "\n",
    "            # iterate through each item in the fourth level (fourth level = specific cases of subgroup (EX. Hypoplasia of the left thyroid lobe))\n",
    "            for item_l4 in fourth_level:\n",
    "                item_l4_id = item_l4['id']\n",
    "                item_l4_title = item_l4['title']\n",
    "                item_l4_subtitle = item_l4['subtitle']\n",
    "                item_l4_thumb = item_l4['thumb']\n",
    "                item_l4_urltitle = item_l4['urltitle']\n",
    "\n",
    "                #print(\"Fetching\", item_l4_title, item_l4_id, item_l4_urltitle)\n",
    "\n",
    "                item_l5_soup = request_data(\n",
    "                    \"https://www.ultrasoundcases.info/{}/\".format(item_l4_urltitle)\n",
    "                )\n",
    "\n",
    "                # img information\n",
    "                fifth_level = item_l5_soup.find(\"div\", {\"class\": \"portfolio\"})\n",
    "                fifth_level = fifth_level.find_all(\"img\")\n",
    "\n",
    "                # patient details\n",
    "                l5_details = item_l5_soup.find(\"div\", {\"class\": \"information\"})\n",
    "                l5_h4 = l5_details.find(\"h4\")\n",
    "                # check if details about case exist\n",
    "                if(l5_h4.text == 'Details'):\n",
    "                    l5_patient_info = l5_details.find_all(\"li\")\n",
    "                    for i, li in enumerate(l5_patient_info):\n",
    "                        details = ''.join(li.find_all(string=True, recursive=False)).strip()\n",
    "                        if i == 0:\n",
    "                            l5_sex = details\n",
    "                        if i == 1:\n",
    "                            l5_age = details\n",
    "                        if i == 2:\n",
    "                            l5_body_part = details\n",
    "                else:\n",
    "                    l5_sex = np.nan\n",
    "                    l5_age = np.nan\n",
    "                    l5_body_part = np.nan\n",
    "                    \n",
    "\n",
    "                # iterate through each item in the case level (fifth level = image details for the specific case)\n",
    "                for item_l5 in fifth_level:  # case level\n",
    "                    item_l5_img_url_src = item_l5[\"src\"]\n",
    "                    item_l5_img_url = \"https://www.ultrasoundcases.info\" + item_l5_img_url_src\n",
    "\n",
    "                    img_soup = request_data(\n",
    "                        item_l5_img_url\n",
    "                    )\n",
    "\n",
    "                    #print(\"Fetching\", item_l5_img_url)\n",
    "\n",
    "                    # prep data for dataframe\n",
    "                    item_l3_url = base_url + item_l2_url_title + item_l3_urltitle\n",
    "                    img_name = item_l5_img_url.split('/')\n",
    "                    item_l5_img_name = img_name[-1]\n",
    "\n",
    "                    if not (item_l5_img_name.endswith(\".jpg\")):\n",
    "                        # not a jpg image, so do not add it, go to the next image\n",
    "                        continue\n",
    "\n",
    "                    # download images here\n",
    "                    if (download_imgs(item_l5_img_url, tgt_path, item_l5_img_name)):\n",
    "                        # if an image is downloaded\n",
    "                        # create row for dataframe\n",
    "                        data_4.loc[len(data_4.index)] = [item_l2_title, item_l3_title, item_l3_url, item_l4_subtitle, item_l5_img_url, item_l5_img_name, l5_sex, l5_age, l5_body_part]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to csv file\n",
    "data_4.to_csv('section4/section4.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B. Section 5 web scraping + download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5 = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scraping the data from HMSS website for section 5 (5.1-5.8)\n",
    "\"\"\"\n",
    "\n",
    "base_url = 'https://www.ultrasoundcases.info/'\n",
    "INIT_URL = \"https://www.ultrasoundcases.info/cases/breast-and-axilla/\"\n",
    "init_soup = request_data(INIT_URL)\n",
    "first_level = init_soup.find_all(\"div\", {\"class\": \"candidate-filter\"})\n",
    "tgt_path = 'imgs/'\n",
    "# make sure file directory exists\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "# go through all first level categories (first level = body system (EX. 5 Breast and Axilla))\n",
    "for item_l1 in first_level:\n",
    "    # get the title from h4>a\n",
    "    item_l1_title = item_l1.find(\"h4\").find(\"a\").text\n",
    "    # only fetch section 4 items\n",
    "    if (item_l1_title != 'Breast and Axilla'):\n",
    "        continue\n",
    "    print('Fetching', item_l1_title)\n",
    "    \n",
    "    # get the second level categories\n",
    "    second_level = item_l1.find(\"ul\").find_all(\"li\")\n",
    "    \n",
    "    # iterate through each item in the second level (second level = group of body system (EX. 5.1 Benign Lesions))\n",
    "    for item_l2 in second_level:\n",
    "        item_l2_title = item_l2.find(\"a\").text\n",
    "        item_l2_url_title = item_l2.find(\"a\")['href']\n",
    "        item_l2_id = item_l2.find(\"a\")[\"data-id\"] \n",
    "\n",
    "        print(\"Fetching\", item_l2_title, item_l2_id)\n",
    "\n",
    "        third_level = requests.get(\n",
    "            \"https://www.ultrasoundcases.info/api/cases/list/\" + str(item_l2_id)\n",
    "        ).json()\n",
    "        \n",
    "        # iterate through each item in the third level (third level = subgroup of group (EX. 5.1.1 Benign cysts with non mass lesions))\n",
    "        for item_l3 in third_level: \n",
    "            item_l3_title = item_l3[\"title\"]\n",
    "            item_l3_urltitle = item_l3[\"urltitle\"]\n",
    "            item_l3_id = item_l3[\"id\"]\n",
    "\n",
    "            print(\"Fetching\", item_l3_title, item_l3_id)\n",
    "\n",
    "            fourth_level = requests.post(\n",
    "                \"https://www.ultrasoundcases.info/api/cases/list/\" +  str(item_l3_id) + '/',\n",
    "                data={\"type\": \"subsubcat\"}\n",
    "            ).json()\n",
    "\n",
    "            # iterate through each item in the fourth level (fourth level = specific cases of subgroup (EX. Benign cysts with non mass lesions))\n",
    "            for item_l4 in fourth_level:\n",
    "                item_l4_id = item_l4['id']\n",
    "                item_l4_title = item_l4['title']\n",
    "                item_l4_subtitle = item_l4['subtitle']\n",
    "                item_l4_thumb = item_l4['thumb']\n",
    "                item_l4_urltitle = item_l4['urltitle']\n",
    "\n",
    "                print(\"Fetching\", item_l4_title, item_l4_id, item_l4_urltitle)\n",
    "\n",
    "                item_l5_soup = request_data(\n",
    "                    \"https://www.ultrasoundcases.info/{}/\".format(item_l4_urltitle)\n",
    "                )\n",
    "\n",
    "                # img information\n",
    "                fifth_level = item_l5_soup.find(\"div\", {\"class\": \"portfolio\"})\n",
    "                fifth_level = fifth_level.find_all(\"img\")\n",
    "\n",
    "                # patient details\n",
    "                l5_details = item_l5_soup.find(\"div\", {\"class\": \"information\"})\n",
    "                l5_h4 = l5_details.find(\"h4\")\n",
    "                \n",
    "                # check if details about case exist\n",
    "                if(l5_h4.text == 'Details'):\n",
    "                    l5_patient_info = l5_details.find_all(\"li\")\n",
    "                    for i, li in enumerate(l5_patient_info):\n",
    "                        details = ''.join(li.find_all(string=True, recursive=False)).strip()\n",
    "                        if i == 0:\n",
    "                            l5_sex = details\n",
    "                        if i == 1:\n",
    "                            l5_age = details\n",
    "                        if i == 2:\n",
    "                            l5_body_part = details\n",
    "                else:\n",
    "                    l5_sex = np.nan\n",
    "                    l5_age = np.nan\n",
    "                    l5_body_part = np.nan\n",
    "                    \n",
    "\n",
    "                # iterate through each item in the case level (fifth level = image details for the specific case)\n",
    "                for item_l5 in fifth_level:  # case level\n",
    "                    item_l5_img_url_src = item_l5[\"src\"]\n",
    "                    item_l5_img_url = \"https://www.ultrasoundcases.info\" + item_l5_img_url_src\n",
    "\n",
    "                    img_soup = request_data(\n",
    "                        item_l5_img_url\n",
    "                    )\n",
    "\n",
    "                    print(\"Fetching\", item_l5_img_url)\n",
    "\n",
    "                    # prep data for dataframe\n",
    "                    item_l3_url = base_url + item_l2_url_title + item_l3_urltitle\n",
    "                    img_name = item_l5_img_url.split('/')\n",
    "                    item_l5_img_name = img_name[-1]\n",
    "\n",
    "                    if not (item_l5_img_name.endswith(\".jpg\")):\n",
    "                        # not a jpg image, so do not add it, go to the next image\n",
    "                        continue\n",
    "\n",
    "                    # download images here\n",
    "                    if (download_imgs(item_l5_img_url, tgt_path, item_l5_img_name)):\n",
    "                        # if an image is downloaded\n",
    "                        # create row for dataframe\n",
    "                        data_5.loc[len(data_5.index)] = [item_l2_title, item_l3_title, item_l3_url, item_l4_subtitle, item_l5_img_url, item_l5_img_name, l5_sex, l5_age, l5_body_part]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to csv file\n",
    "data_5.to_csv('section5/section5.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1C. Section 7 web scraping + download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_7 = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scraping the data from HMSS website for section 7 (7.2-7.5)\n",
    "\"\"\"\n",
    "\n",
    "base_url = 'https://www.ultrasoundcases.info/'\n",
    "INIT_URL = \"https://www.ultrasoundcases.info/cases/musculo-skeletal-bone-muscle-nerves-and-other-soft-tissues/\"\n",
    "init_soup = request_data(INIT_URL)\n",
    "first_level = init_soup.find_all(\"div\", {\"class\": \"candidate-filter\"})\n",
    "tgt_path = 'imgs/'\n",
    "# make sure file directory exists\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "# go through all first level categories (first level = body system (EX. 7 Musculoskeletal, bone, muscle, nerves and other soft tissues))\n",
    "for item_l1 in first_level:\n",
    "    # get the title from h4>a\n",
    "    item_l1_title = item_l1.find(\"h4\").find(\"a\").text\n",
    "    # only fetch section 4 items\n",
    "    if (item_l1_title != 'Musculoskeletal, bone, muscle, nerves and other soft tissues'):\n",
    "        continue\n",
    "    print('Fetching', item_l1_title)\n",
    "    \n",
    "    # get the second level categories\n",
    "    second_level = item_l1.find(\"ul\").find_all(\"li\")\n",
    "    \n",
    "    # iterate through each item in the second level (second level = group of body system (EX. 7.2 Muscle))\n",
    "    for item_l2 in second_level:\n",
    "        item_l2_title = item_l2.find(\"a\").text\n",
    "        item_l2_url_title = item_l2.find(\"a\")['href']\n",
    "        item_l2_id = item_l2.find(\"a\")[\"data-id\"] \n",
    "        \n",
    "        # skip section 7.1\n",
    "        if item_l2_title == '7.1 Bone':\n",
    "            continue\n",
    "\n",
    "        print(\"Fetching\", item_l2_title, item_l2_id)\n",
    "\n",
    "        third_level = requests.get(\n",
    "            \"https://www.ultrasoundcases.info/api/cases/list/\" + str(item_l2_id)\n",
    "        ).json()\n",
    "        \n",
    "        # iterate through each item in the third level (third level = subgroup of group (EX. 7.2.1 Muscle ruptures lower extremity upper leg))\n",
    "        for item_l3 in third_level: \n",
    "            item_l3_title = item_l3[\"title\"]\n",
    "            item_l3_urltitle = item_l3[\"urltitle\"]\n",
    "            item_l3_id = item_l3[\"id\"]\n",
    "\n",
    "            print(\"Fetching\", item_l3_title, item_l3_id)\n",
    "\n",
    "            fourth_level = requests.post(\n",
    "                \"https://www.ultrasoundcases.info/api/cases/list/\" +  str(item_l3_id) + '/',\n",
    "                data={\"type\": \"subsubcat\"}\n",
    "            ).json()\n",
    "\n",
    "            # iterate through each item in the fourth level (fourth level = specific cases of subgroup (EX. Lower extremety: upper leg))\n",
    "            for item_l4 in fourth_level:\n",
    "                item_l4_id = item_l4['id']\n",
    "                item_l4_title = item_l4['title']\n",
    "                item_l4_subtitle = item_l4['subtitle']\n",
    "                item_l4_thumb = item_l4['thumb']\n",
    "                item_l4_urltitle = item_l4['urltitle']\n",
    "\n",
    "                print(\"Fetching\", item_l4_title, item_l4_id, item_l4_urltitle)\n",
    "\n",
    "                item_l5_soup = request_data(\n",
    "                    \"https://www.ultrasoundcases.info/{}/\".format(item_l4_urltitle)\n",
    "                )\n",
    "\n",
    "                # img information\n",
    "                fifth_level = item_l5_soup.find(\"div\", {\"class\": \"portfolio\"})\n",
    "                fifth_level = fifth_level.find_all(\"img\")\n",
    "\n",
    "                # patient details\n",
    "                l5_details = item_l5_soup.find(\"div\", {\"class\": \"information\"})\n",
    "                l5_h4 = l5_details.find(\"h4\")\n",
    "\n",
    "                # check if details about case exist\n",
    "                if(l5_h4.text == 'Details'):\n",
    "                    l5_patient_info = l5_details.find_all(\"li\")\n",
    "                    for i, li in enumerate(l5_patient_info):\n",
    "                        details = ''.join(li.find_all(string=True, recursive=False)).strip()\n",
    "                        if i == 0:\n",
    "                            l5_sex = details\n",
    "                        if i == 1:\n",
    "                            l5_age = details\n",
    "                        if i == 2:\n",
    "                            l5_body_part = details\n",
    "                else:\n",
    "                    l5_sex = np.nan\n",
    "                    l5_age = np.nan\n",
    "                    l5_body_part = np.nan\n",
    "                    \n",
    "\n",
    "                # iterate through each item in the case level (fifth level = image details for the specific case)\n",
    "                for item_l5 in fifth_level:  # case level\n",
    "                    item_l5_img_url_src = item_l5[\"src\"]\n",
    "                    item_l5_img_url = \"https://www.ultrasoundcases.info\" + item_l5_img_url_src\n",
    "\n",
    "                    img_soup = request_data(\n",
    "                        item_l5_img_url\n",
    "                    )\n",
    "\n",
    "                    print(\"Fetching\", item_l5_img_url)\n",
    "\n",
    "                    # prep data for dataframe\n",
    "                    item_l3_url = base_url + item_l2_url_title + item_l3_urltitle\n",
    "                    img_name = item_l5_img_url.split('/')\n",
    "                    item_l5_img_name = img_name[-1]\n",
    "\n",
    "                    if not (item_l5_img_name.endswith(\".jpg\")):\n",
    "                        # not a jpg image, so do not add it, go to the next image\n",
    "                        continue\n",
    "\n",
    "                    # download images here\n",
    "                    if (download_imgs(item_l5_img_url, tgt_path, item_l5_img_name)):\n",
    "                        # if an image is downloaded\n",
    "                        # create row for dataframe\n",
    "                        data_7.loc[len(data_7.index)] = [item_l2_title, item_l3_title, item_l3_url, item_l4_subtitle, item_l5_img_url, item_l5_img_name, l5_sex, l5_age, l5_body_part]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to csv file\n",
    "data_7.to_csv('section7/section7.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D. Section 8 web scraping + download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_8 = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scraping the data from HMSS website for section 8 (8.1-8.4)\n",
    "\"\"\"\n",
    "\n",
    "base_url = 'https://www.ultrasoundcases.info/'\n",
    "INIT_URL = \"https://www.ultrasoundcases.info/cases/thorax/\"\n",
    "init_soup = request_data(INIT_URL)\n",
    "first_level = init_soup.find_all(\"div\", {\"class\": \"candidate-filter\"})\n",
    "tgt_path = 'imgs/'\n",
    "# make sure file directory exists\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "# go through all first level categories (first level = body system (EX. 8 Thorax))\n",
    "for item_l1 in first_level:\n",
    "    # get the title from h4>a\n",
    "    item_l1_title = item_l1.find(\"h4\").find(\"a\").text\n",
    "    # only fetch section 4 items\n",
    "    if (item_l1_title != 'Thorax'):\n",
    "        continue\n",
    "    print('Fetching', item_l1_title)\n",
    "    \n",
    "    # get the second level categories\n",
    "    second_level = item_l1.find(\"ul\").find_all(\"li\")\n",
    "    \n",
    "    # iterate through each item in the second level (second level = group of body system (EX. 8.1 Pulmonary pathology))\n",
    "    for item_l2 in second_level:\n",
    "        item_l2_title = item_l2.find(\"a\").text\n",
    "        item_l2_url_title = item_l2.find(\"a\")['href']\n",
    "        item_l2_id = item_l2.find(\"a\")[\"data-id\"] \n",
    "\n",
    "        print(\"Fetching\", item_l2_title, item_l2_id)\n",
    "\n",
    "        third_level = requests.get(\n",
    "            \"https://www.ultrasoundcases.info/api/cases/list/\" + str(item_l2_id)\n",
    "        ).json()\n",
    "        \n",
    "        # iterate through each item in the third level (third level = subgroup of group (EX. 8.1.1 Pneumonia and air space consolidation))\n",
    "        for item_l3 in third_level: \n",
    "            item_l3_title = item_l3[\"title\"]\n",
    "            item_l3_urltitle = item_l3[\"urltitle\"]\n",
    "            item_l3_id = item_l3[\"id\"]\n",
    "\n",
    "            print(\"Fetching\", item_l3_title, item_l3_id)\n",
    "\n",
    "            fourth_level = requests.post(\n",
    "                \"https://www.ultrasoundcases.info/api/cases/list/\" +  str(item_l3_id) + '/',\n",
    "                data={\"type\": \"subsubcat\"}\n",
    "            ).json()\n",
    "\n",
    "            # iterate through each item in the fourth level (fourth level = specific cases of subgroup (EX. Pneumonia and air space consolidation))\n",
    "            for item_l4 in fourth_level:\n",
    "                item_l4_id = item_l4['id']\n",
    "                item_l4_title = item_l4['title']\n",
    "                item_l4_subtitle = item_l4['subtitle']\n",
    "                item_l4_thumb = item_l4['thumb']\n",
    "                item_l4_urltitle = item_l4['urltitle']\n",
    "\n",
    "                print(\"Fetching\", item_l4_title, item_l4_id, item_l4_urltitle)\n",
    "\n",
    "                item_l5_soup = request_data(\n",
    "                    \"https://www.ultrasoundcases.info/{}/\".format(item_l4_urltitle)\n",
    "                )\n",
    "\n",
    "                # img information\n",
    "                fifth_level = item_l5_soup.find(\"div\", {\"class\": \"portfolio\"})\n",
    "                fifth_level = fifth_level.find_all(\"img\")\n",
    "\n",
    "                # patient details\n",
    "                l5_details = item_l5_soup.find(\"div\", {\"class\": \"information\"})\n",
    "                l5_h4 = l5_details.find(\"h4\")\n",
    "                \n",
    "                # check if details about case exist\n",
    "                if(l5_h4.text == 'Details'):\n",
    "                    l5_patient_info = l5_details.find_all(\"li\")\n",
    "                    for i, li in enumerate(l5_patient_info):\n",
    "                        details = ''.join(li.find_all(string=True, recursive=False)).strip()\n",
    "                        if i == 0:\n",
    "                            l5_sex = details\n",
    "                        if i == 1:\n",
    "                            l5_age = details\n",
    "                        if i == 2:\n",
    "                            l5_body_part = details\n",
    "                else:\n",
    "                    l5_sex = np.nan\n",
    "                    l5_age = np.nan\n",
    "                    l5_body_part = np.nan\n",
    "                    \n",
    "\n",
    "                # iterate through each item in the case level (fifth level = image details for the specific case)\n",
    "                for item_l5 in fifth_level:  # case level\n",
    "                    item_l5_img_url_src = item_l5[\"src\"]\n",
    "                    item_l5_img_url = \"https://www.ultrasoundcases.info\" + item_l5_img_url_src\n",
    "\n",
    "                    img_soup = request_data(\n",
    "                        item_l5_img_url\n",
    "                    )\n",
    "\n",
    "                    print(\"Fetching\", item_l5_img_url)\n",
    "\n",
    "                    # prep data for dataframe\n",
    "                    item_l3_url = base_url + item_l2_url_title + item_l3_urltitle\n",
    "                    img_name = item_l5_img_url.split('/')\n",
    "                    item_l5_img_name = img_name[-1]\n",
    "                    \n",
    "                    if not (item_l5_img_name.endswith(\".jpg\")):\n",
    "                        # not a jpg image, so do not add it, go to the next image\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    # download images here\n",
    "                    if (download_imgs(item_l5_img_url, tgt_path, item_l5_img_name)): \n",
    "                        # if an image is downloaded\n",
    "                        # create row for dataframe\n",
    "                        data_8.loc[len(data_8.index)] = [item_l2_title, item_l3_title, item_l3_url, item_l4_subtitle, item_l5_img_url, item_l5_img_name, l5_sex, l5_age, l5_body_part]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item_l5_img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to csv file\n",
    "data_8.to_csv('section8/section8.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: combine .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# section 4 csv\n",
    "section4 = 'section4/section4.csv'\n",
    "df_4 = pd.read_csv(section4)\n",
    "#df_4.head()\n",
    "\n",
    "# section 5 csv\n",
    "section5 = 'section5/section5.csv'\n",
    "df_5 = pd.read_csv(section5)\n",
    "\n",
    "# section 7 csv\n",
    "section7 = 'section7/section7.csv'\n",
    "df_7 = pd.read_csv(section7)\n",
    "\n",
    "# section 8 csv\n",
    "section8 = 'section8/section8.csv'\n",
    "df_8 = pd.read_csv(section8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the dataframes to one dataframe\n",
    "df_all = [df_4, df_5, df_7, df_8]\n",
    "df = pd.DataFrame()\n",
    "df = pd.concat(df_all, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image folders and load image urls from HMSS.csv\n",
    "tgt_path = 'imgs/'\n",
    "\n",
    "if not os.path.exists(tgt_path):\n",
    "    os.makedirs(tgt_path)\n",
    "\n",
    "urllinkimage = df['img url']\n",
    "\n",
    "img_names = df['img name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download images\n",
    "def download_imgs(urllink, tgt_path, names):\n",
    "    for i, img_url in enumerate(urllink.values ):\n",
    "        filename = '{}'.format(names[i])\n",
    "        full_tgt_path = '{}{}'.format(tgt_path, filename)\n",
    "        print(img_url)\n",
    "        urllib.request.urlretrieve(img_url, full_tgt_path)\n",
    "        \n",
    "        print('{} - {} saved to {}.'.format(i, filename, full_tgt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_imgs(urllinkimage, tgt_path, img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "946e85727053cf3aa9c9f3f7fedca17119c324c882ebd65fbc35521c9647d1eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
